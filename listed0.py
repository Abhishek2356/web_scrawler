# -*- coding: utf-8 -*-
"""Listed0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19UgTJvx5nl_msxqXJctJyUmZlS_gqOLp
"""

import requests
from bs4 import BeautifulSoup

# url ="https://in.search.yahoo.com/search;_ylt=AwrKAmNgqz1k2DIAa1y7HAx.;_ylu=Y29sbwNzZzMEcG9zAzEEdnRpZAMEc2VjA3BhZ2luYXRpb24-?p=site%3Ayoutube.com+openinapp.co&pz=7&vm=r&type=E211IN885G91710&fr=mcafee&fr2=sb-top&b=0&pz=7&xargs=0"

# news = url[0:url.index("&b")+3]+str(14)+url[url.index("&b")+3+len(str(14)):]
# news

# response = requests.get(
#     url
# )

# beauty_code = BeautifulSoup(response.content, "html.parser")

# anchor_list = beauty_code.find_all("a")
# # anchor_list

# url ="https://www.google.com/search?q=site:youtube.com+openinapp.co&sxsrf=APwXEddRpXap1GiJ_yj18Q7n9st6_7mJQQ:1681764613380&ei=BbE9ZKnnFp6k2roPhf2y6Aw&start=10&sa=N&ved=2ahUKEwjp_YCz5bH-AhUeklYBHYW-DM0Q8tMDegQIEhAE&biw=1536&bih=792&dpr=1.25"

url ="https://www.bing.com/search?q=site%3ayoutube.com+openinapp.co&PC=U316&first=0&FORM=PERE1"



def yahoo_site(url, next_page) -> list:

  url = url[0:url.index("first")+6]+str(next_page)+url[url.index("first")+5+len(str(next_page)):]
  response = requests.get(url)
  beauty_code = BeautifulSoup(response.content, "html.parser")
  anchor_list = beauty_code.find_all("a")
  return anchor_list

def get_link_list():
  next_page = 0
  count = 0
  condition = True 
  while condition:
    next_page+=11
    if next_page>12000:
      print(" Data has more then page 100000 ...")
      condition = False
    response = yahoo_site(url, next_page)
    for element in response:
      string_url = element.get("href","")
      if isinstance(string_url, str) and string_url.__contains__("watch"):
        with open("./other.txt","a+") as files:
          files.write(string_url)
          files.write("\n")
      else:
        continue 
    count+=1
    print(f"Page of {count} data is completed....")

get_link_list()

import pandas as pd 


with open("./other.txt","r+") as files:
  all_data = files.readlines()

  data_pandas = pd.DataFrame(all_data)
  data_pandas.to_csv("./listed.csv", index=False)
  print(data_pandas)

